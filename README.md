# O-RAAC
Offline Risk-Averse Actor-Critic (O-RAAC). A model-free RL algorithm for risk-averse RL in a fully offline setting.
Code to reproduce the results in: <a href= "https://openreview.net/forum?id=TBIzh9b5eaz"> Risk-Averse Offline Reinforcement Learning

O-RAAC can be installed by cloning the repository as follows:
```
git clone git@github.com:nuria95/O-RAAC.git
cd O-RAAC
pip install -e .
```

In order to run O-RAAC you need to install D4RL (follow the instruction in https://github.com/rail-berkeley/d4rl).


